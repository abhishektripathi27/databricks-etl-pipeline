{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f186d88f-edd4-4a14-86d1-3c63b51ada8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c76982d-33af-4190-8a44-e1eb0b861850",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./000_setup_storage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af2db7d-26ae-46c6-abf2-1a635a1caa0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source file from GitHub (raw CSV)\n",
    "orders_csv_url = \"https://raw.githubusercontent.com/abhishektripathi27/databricks-etl-pipeline/main/data/orders.csv\"\n",
    "\n",
    "# Raw landing path in ADLS\n",
    "raw_orders_path = f\"{raw_path}/orders/orders.csv\"\n",
    "\n",
    "# Bronze Delta table path in ADLS\n",
    "bronze_orders_path = f\"{bronze_path}/orders\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfee773-1487-49fd-922d-498c3042b89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(orders_csv_url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "dbutils.fs.put(raw_orders_path, response.text, overwrite=True)\n",
    "\n",
    "print(\"orders.csv uploaded to RAW zone:\", raw_orders_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350ec5ca-0e53-49f7-9176-4554a156906b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6805734-d4e4-4b4a-88e5-696f885af111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze_orders\n",
    "USING DELTA\n",
    "LOCATION '{bronze_orders_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Registered table: bronze_orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db002a3f-5f74-41c9-a883-cbf1d5d93b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = spark.read.format(\"delta\").load(bronze_orders_path)\n",
    "\n",
    "print(\"Bronze count:\", bronze_df.count())\n",
    "display(bronze_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895fa1ec-75d8-4478-bd8c-f999009b4885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_table_path = bronze_path + \"orders/\"\n",
    "bronze_orders = spark.read.format(\"delta\").load(bronze_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6526773-0bee-492e-87f5-0d20a85a8f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Databricks notebook source\n",
    "# # -----------------------------\n",
    "# # STEP 0: Absolute ABFS paths\n",
    "# raw_path        = \"abfss://datalake@etldatalakeabhi.dfs.core.windows.net/raw/\"\n",
    "# bronze_path     = \"abfss://datalake@etldatalakeabhi.dfs.core.windows.net/bronze/\"\n",
    "# checkpoint_path = \"abfss://datalake@etldatalakeabhi.dfs.core.windows.net/checkpoint/\"\n",
    "\n",
    "# # Optional: Test access\n",
    "# display(dbutils.fs.ls(raw_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0ae8aa-7984-4d04-9229-ea13f0ff0ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from GitHub\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "pdf = pd.read_csv(github_url)\n",
    "bronze_df = spark.createDataFrame(pdf).withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "# Bad/Good rows separation using min_order_amount and cancelled_status\n",
    "valid_condition = (\n",
    "    (col(\"order_id\").isNotNull()) &\n",
    "    (col(\"status\").isNotNull()) &\n",
    "    ((col(\"amount\").isNotNull()) | (col(\"status\") == cancelled_status))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37125151-de4f-414f-b55c-8fef4d04d4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 2: Separate good vs bad rows\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Conditional validation\n",
    "valid_condition = (\n",
    "    (col(\"order_id\").isNotNull()) &\n",
    "    (col(\"status\").isNotNull()) &\n",
    "    ((col(\"amount\").isNotNull()) | (col(\"status\") == \"cancelled\"))\n",
    ")\n",
    "\n",
    "# Good rows → Bronze\n",
    "good_bronze = bronze_df.filter(valid_condition)\n",
    "\n",
    "good_bronze.show()\n",
    "\n",
    "# Bad rows → RAW quarantine\n",
    "bad_rows = bronze_df.filter(~valid_condition) \\\n",
    "    .withColumn(\"bad_data_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\n",
    "        \"error_reason\",\n",
    "        when(col(\"order_id\").isNull(), \"Missing order_id\")\n",
    "        .when(col(\"status\").isNull(), \"Missing status\")\n",
    "        .when((col(\"amount\").isNull()) & (col(\"status\") != \"cancelled\"), \"Amount missing for non-cancelled order\")\n",
    "        .otherwise(\"Unknown error\")\n",
    "    )\n",
    "\n",
    "bad_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4f6d62-2150-4e02-9d15-461b7393f5db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 3: Write bad rows to RAW quarantine folder\n",
    "bad_rows.write.format(\"delta\").mode(\"append\").save(raw_path + \"bad_orders/\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ws_databricks_etl.bronze.bad_orders\n",
    "USING DELTA\n",
    "LOCATION 'abfss://datalake@etldatalakeabhi.dfs.core.windows.net/raw/bad_orders/'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b5cec9-23ed-480c-b5b1-f7e7f79bb481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Optional: Register in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6c06e3-a62a-414c-a897-1872b36a0a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 4: Upsert good rows into Bronze Delta\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "bronze_table_path = bronze_path + \"orders/\"\n",
    "\n",
    "# Create table if doesn't exist\n",
    "if not DeltaTable.isDeltaTable(spark, bronze_table_path):\n",
    "    good_bronze.write.format(\"delta\").mode(\"overwrite\").save(bronze_table_path)\n",
    "\n",
    "# Merge / upsert\n",
    "bronze_table = DeltaTable.forPath(spark, bronze_table_path)\n",
    "bronze_table.alias(\"bronze\").merge(\n",
    "    good_bronze.alias(\"raw\"),\n",
    "    \"bronze.order_id = raw.order_id\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "# Register table in Unity Catalog\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ws_databricks_etl.bronze.orders\n",
    "USING DELTA\n",
    "LOCATION '{bronze_table_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a539cd-572d-419f-b8d8-fa58d4abc2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Read CSV from raw folder\n",
    "orders_df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(raw_path + \"orders/orders.csv\")\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Show first few rows\n",
    "orders_df.display()\n",
    "\n",
    "# Write to Bronze Delta\n",
    "orders_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path + \"orders\")\n",
    "\n",
    "# Register in Unity Catalog\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ws_databricks_etl.default.bronze_orders\n",
    "USING DELTA\n",
    "LOCATION '{bronze_path}orders'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Bronze Orders table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c6d63a-752b-4877-8e66-15b0564a852e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- select * from ws_databricks_etl.bronze.bad_orders\n",
    "select * from ws_databricks_etl.bronze.orders"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7064402793545861,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "001_ingest_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
