{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6526773-0bee-492e-87f5-0d20a85a8f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 0: Absolute ABFS paths\n",
    "raw_path        = \"abfss://datalake@etldatalakeabhi.dfs.core.windows.net/raw/\"\n",
    "bronze_path     = \"abfss://datalake@etldatalakeabhi.dfs.core.windows.net/bronze/\"\n",
    "checkpoint_path = \"abfss://datalake@etldatalakeabhi.dfs.core.windows.net/checkpoint/\"\n",
    "\n",
    "# Optional: Test access\n",
    "display(dbutils.fs.ls(raw_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0ae8aa-7984-4d04-9229-ea13f0ff0ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 1: Read RAW CSV from GitHub (or RAW folder)\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "github_url = \"https://raw.githubusercontent.com/abhishektripathi27/databricks-etl-pipeline/main/data/orders.csv\"\n",
    "pdf = pd.read_csv(github_url)\n",
    "\n",
    "bronze_df = SparkSession.builder.getOrCreate().createDataFrame(pdf)\n",
    "bronze_df = bronze_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "display(bronze_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37125151-de4f-414f-b55c-8fef4d04d4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 2: Separate good vs bad rows\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Conditional validation\n",
    "valid_condition = (\n",
    "    (col(\"order_id\").isNotNull()) &\n",
    "    (col(\"status\").isNotNull()) &\n",
    "    ((col(\"amount\").isNotNull()) | (col(\"status\") == \"cancelled\"))\n",
    ")\n",
    "\n",
    "# Good rows → Bronze\n",
    "good_bronze = bronze_df.filter(valid_condition)\n",
    "\n",
    "good_bronze.show()\n",
    "\n",
    "# Bad rows → RAW quarantine\n",
    "bad_rows = bronze_df.filter(~valid_condition) \\\n",
    "    .withColumn(\"bad_data_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\n",
    "        \"error_reason\",\n",
    "        when(col(\"order_id\").isNull(), \"Missing order_id\")\n",
    "        .when(col(\"status\").isNull(), \"Missing status\")\n",
    "        .when((col(\"amount\").isNull()) & (col(\"status\") != \"cancelled\"), \"Amount missing for non-cancelled order\")\n",
    "        .otherwise(\"Unknown error\")\n",
    "    )\n",
    "\n",
    "bad_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4f6d62-2150-4e02-9d15-461b7393f5db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 3: Write bad rows to RAW quarantine folder\n",
    "bad_rows.write.format(\"delta\").mode(\"append\").save(raw_path + \"bad_orders/\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ws_databricks_etl.bronze.bad_orders\n",
    "USING DELTA\n",
    "LOCATION 'abfss://datalake@etldatalakeabhi.dfs.core.windows.net/raw/bad_orders/'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b5cec9-23ed-480c-b5b1-f7e7f79bb481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Optional: Register in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6c06e3-a62a-414c-a897-1872b36a0a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# -----------------------------\n",
    "# STEP 4: Upsert good rows into Bronze Delta\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "bronze_table_path = bronze_path + \"orders/\"\n",
    "\n",
    "# Create table if doesn't exist\n",
    "if not DeltaTable.isDeltaTable(spark, bronze_table_path):\n",
    "    good_bronze.write.format(\"delta\").mode(\"overwrite\").save(bronze_table_path)\n",
    "\n",
    "# Merge / upsert\n",
    "bronze_table = DeltaTable.forPath(spark, bronze_table_path)\n",
    "bronze_table.alias(\"bronze\").merge(\n",
    "    good_bronze.alias(\"raw\"),\n",
    "    \"bronze.order_id = raw.order_id\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "# Register table in Unity Catalog\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ws_databricks_etl.bronze.orders\n",
    "USING DELTA\n",
    "LOCATION '{bronze_table_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1a539cd-572d-419f-b8d8-fa58d4abc2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Read CSV from raw folder\n",
    "orders_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(raw_path + \"orders/orders.csv\")\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Show first few rows\n",
    "orders_df.display()\n",
    "\n",
    "# Write to Bronze Delta\n",
    "orders_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path + \"orders\")\n",
    "\n",
    "# Register in Unity Catalog\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ws_databricks_etl.default.bronze_orders\n",
    "USING DELTA\n",
    "LOCATION '{bronze_path}orders'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Bronze Orders table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c6d63a-752b-4877-8e66-15b0564a852e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- select * from ws_databricks_etl.bronze.bad_orders\n",
    "select * from ws_databricks_etl.bronze.orders"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7064402793545861,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "001_ingest_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
