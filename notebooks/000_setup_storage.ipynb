{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e640962-021f-4c8d-a522-72539ba45956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Storage and environment\n",
    "dbutils.widgets.text(\"storage_account_name\", \"etldatalakeabhi\")\n",
    "dbutils.widgets.text(\"container_name\", \"datalake\")\n",
    "dbutils.widgets.text(\"env\", \"dev\")\n",
    "\n",
    "# Paths\n",
    "dbutils.widgets.text(\"github_url\", \"https://raw.githubusercontent.com/abhishektripathi27/databricks-etl-pipeline/main/data/orders.csv\")\n",
    "dbutils.widgets.text(\"min_order_amount\", \"0.01\")\n",
    "dbutils.widgets.text(\"cancelled_status\", \"cancelled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d5d532-17a5-40da-afd1-b8f523587113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = dbutils.widgets.get(\"storage_account_name\")\n",
    "container_name  = dbutils.widgets.get(\"container_name\")\n",
    "env             = dbutils.widgets.get(\"env\")\n",
    "\n",
    "# Dynamic ADLS paths\n",
    "raw_path        = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/raw/\"\n",
    "bronze_path     = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/bronze/\"\n",
    "silver_path     = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/\"\n",
    "gold_path       = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/gold/\"\n",
    "checkpoint_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/checkpoint/\"\n",
    "github_url      = dbutils.widgets.get(\"github_url\")\n",
    "min_order_amount = float(dbutils.widgets.get(\"min_order_amount\"))\n",
    "cancelled_status = dbutils.widgets.get(\"cancelled_status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a76fa46-f68d-42e2-aedb-dd72c8ddfc9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"etldatalakeabhi\"\n",
    "container = \"datalake\"\n",
    "\n",
    "raw_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/raw/\"\n",
    "bronze_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/\"\n",
    "silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/\"\n",
    "gold_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/gold/\"\n",
    "checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoint/\"\n",
    "\n",
    "print(\"RAW:\", raw_path)\n",
    "print(\"BRONZE:\", bronze_path)\n",
    "print(\"SILVER:\", silver_path)\n",
    "print(\"GOLD:\", gold_path)\n",
    "print(\"CHECKPOINT:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d1a936-8148-4afa-b7a7-4195fb221c64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unity Catalog Setup Guide"
    }
   },
   "source": [
    "# Unity Catalog External Location Setup Guide\n",
    "\n",
    "To access your Azure storage (`etldatalakeabhi.dfs.core.windows.net`) on serverless compute, you need to set up Unity Catalog external locations.\n",
    "\n",
    "## Prerequisites (Azure Portal Steps)\n",
    "\n",
    "### Step 1: Create an Access Connector for Azure Databricks\n",
    "\n",
    "1. Log in to the **Azure Portal**\n",
    "2. Click **+ Create a resource**\n",
    "3. Search for **\"Access Connector for Azure Databricks\"** and select it\n",
    "4. Click **Create** and fill in:\n",
    "   * **Subscription**: Your Azure subscription\n",
    "   * **Resource Group**: Your resource group (or create new)\n",
    "   * **Name**: `databricks-access-connector-etl` (or your preferred name)\n",
    "   * **Region**: Same region as your storage account\n",
    "5. On the **Managed Identity** tab:\n",
    "   * Set **Status** to **On** (for system-assigned managed identity)\n",
    "6. Click **Review + create**, then **Create**\n",
    "7. Once deployed, go to the resource and **copy the Resource ID**\n",
    "   * Format: `/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Databricks/accessConnectors/<connector-name>`\n",
    "\n",
    "### Step 2: Grant Storage Access to the Managed Identity\n",
    "\n",
    "1. In Azure Portal, navigate to your storage account: **`etldatalakeabhi`**\n",
    "2. Go to **Access Control (IAM)**\n",
    "3. Click **+ Add** → **Add role assignment**\n",
    "4. Select role: **Storage Blob Data Contributor** (or Owner if you need full access)\n",
    "5. Click **Next**\n",
    "6. Select **Managed identity**\n",
    "7. Click **+ Select members**\n",
    "8. Find and select your access connector: **`databricks-access-connector-etl`**\n",
    "9. Click **Review + assign**\n",
    "\n",
    "### Step 3: Configure Network Access (if storage has firewall enabled)\n",
    "\n",
    "If your storage account has network restrictions:\n",
    "1. Go to your storage account → **Networking**\n",
    "2. Under **Exceptions**, enable: **\"Allow Azure services on the trusted services list to access this storage account\"**\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Once you complete the Azure Portal steps above, proceed to the next cells to create the storage credential and external locations in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e0d70c-2374-4fcb-8a5f-0dffc620d30f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Storage Credential"
    }
   },
   "source": [
    "## Create Storage Credential in Databricks\n",
    "\n",
    "After completing the Azure Portal steps, you have two options:\n",
    "\n",
    "### Option A: Using the Databricks UI (Recommended for first-time setup)\n",
    "\n",
    "1. In your Databricks workspace, click **Catalog** in the left sidebar\n",
    "2. Click **External Data** button → **Credentials** tab\n",
    "3. Click **Create credential**\n",
    "4. Select **Storage credential**\n",
    "5. Fill in:\n",
    "   * **Credential Type**: Azure Managed Identity\n",
    "   * **Name**: `azure_storage_credential_etl`\n",
    "   * **Access Connector Resource ID**: Paste the Resource ID from Azure Portal\n",
    "6. Click **Create**\n",
    "\n",
    "### Option B: Using SQL (run the cell below)\n",
    "\n",
    "Replace `<YOUR_ACCESS_CONNECTOR_RESOURCE_ID>` with the actual Resource ID from Azure Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f57d490-640b-4085-b354-7eaad29217d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Python: Create Storage Credential via SDK"
    }
   },
   "outputs": [],
   "source": [
    "# Create storage credential using Databricks SDK\n",
    "# This requires the databricks-sdk package\n",
    "\n",
    "try:\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service.catalog import AzureManagedIdentity\n",
    "    \n",
    "    # Initialize the workspace client (uses notebook context for auth)\n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    # Create the storage credential\n",
    "    credential = w.storage_credentials.create(\n",
    "        name=\"azure_storage_credential_etl\",\n",
    "        azure_managed_identity=AzureManagedIdentity(\n",
    "            access_connector_id=\"/subscriptions/9cc5a0bc-59a0-41c7-9f55-81c0cbefd7ca/resourceGroups/rg-databricks/providers/Microsoft.Databricks/accessConnectors/access_connector_databricks\"\n",
    "        ),\n",
    "        comment=\"Storage credential for ETL data lake\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Storage credential created successfully!\")\n",
    "    print(f\"  Name: {credential.name}\")\n",
    "    print(f\"  ID: {credential.id}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ Databricks SDK not installed.\")\n",
    "    print(\"Install it with: %pip install databricks-sdk\")\n",
    "    print(\"\\nAlternatively, use the CLI or UI method described in Cell 5.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating storage credential: {e}\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"1. You don't have CREATE STORAGE CREDENTIAL privilege (need metastore admin)\")\n",
    "    print(\"2. The access connector doesn't exist in Azure Portal\")\n",
    "    print(\"3. The access connector ID is incorrect\")\n",
    "    print(\"\\nPlease use the UI method or contact your workspace administrator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5cc401-2240-443a-813e-e6afdd1b5f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Storage credential created successfully!\n",
    "  Name: azure_storage_credential_etl\n",
    "  ID: bcd1d9c0-8d1f-42c0-9a71-1f83d4d7e649"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1c3a37-f869-40f8-a7ce-26250fc14dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04b1c78b-a25a-407d-8c35-fa9451306bc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Alternative: Check Existing Credentials"
    }
   },
   "outputs": [],
   "source": [
    "# If you created the credential via UI, verify it exists\n",
    "try:\n",
    "    credentials = spark.sql(\"SHOW STORAGE CREDENTIALS\").collect()\n",
    "    if credentials:\n",
    "        print(\"✓ Available Storage Credentials:\")\n",
    "        for cred in credentials:\n",
    "            print(f\"  - {cred[0]}\")\n",
    "    else:\n",
    "        print(\"⚠ No storage credentials found.\")\n",
    "        print(\"Please create one using the UI (see Cell 4) or SQL (Cell 5)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nYou may need metastore admin privileges to view/create storage credentials.\")\n",
    "    print(\"Please contact your workspace administrator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9d7b8c-15fe-4606-8201-a50d90aa4c28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create External Locations"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create external locations for each layer of your data lake\n",
    "-- These map to the paths you defined in Cell 1\n",
    "-- Make sure the storage credential 'azure_storage_credential_etl' exists first\n",
    "\n",
    "-- RAW layer\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS etl_raw_location\n",
    "URL 'abfss://datalake@etldatalakeabhi.dfs.core.windows.net/raw/'\n",
    "WITH (STORAGE CREDENTIAL azure_storage_credential_etl)\n",
    "COMMENT 'Raw data landing zone';\n",
    "\n",
    "-- BRONZE layer\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS etl_bronze_location\n",
    "URL 'abfss://datalake@etldatalakeabhi.dfs.core.windows.net/bronze/'\n",
    "WITH (STORAGE CREDENTIAL azure_storage_credential_etl)\n",
    "COMMENT 'Bronze layer - raw ingested data';\n",
    "\n",
    "-- SILVER layer\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS etl_silver_location\n",
    "URL 'abfss://datalake@etldatalakeabhi.dfs.core.windows.net/silver/'\n",
    "WITH (STORAGE CREDENTIAL azure_storage_credential_etl)\n",
    "COMMENT 'Silver layer - cleaned and validated data';\n",
    "\n",
    "-- GOLD layer\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS etl_gold_location\n",
    "URL 'abfss://datalake@etldatalakeabhi.dfs.core.windows.net/gold/'\n",
    "WITH (STORAGE CREDENTIAL azure_storage_credential_etl)\n",
    "COMMENT 'Gold layer - business-level aggregates';\n",
    "\n",
    "-- CHECKPOINT location\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS etl_checkpoint_location\n",
    "URL 'abfss://datalake@etldatalakeabhi.dfs.core.windows.net/checkpoint/'\n",
    "WITH (STORAGE CREDENTIAL azure_storage_credential_etl)\n",
    "COMMENT 'Checkpoint location for streaming jobs';\n",
    "\n",
    "-- Verify all external locations were created\n",
    "SHOW EXTERNAL LOCATIONS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9eae8ea-a60a-4b24-bfd5-beb0497cee7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grant Permissions"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Grant yourself permissions to use these external locations\n",
    "-- This allows you to create tables and read/write data in these locations\n",
    "\n",
    "GRANT ALL PRIVILEGES ON EXTERNAL LOCATION etl_raw_location TO `tripathiabhi@hotmail.com`;\n",
    "GRANT ALL PRIVILEGES ON EXTERNAL LOCATION etl_bronze_location TO `tripathiabhi@hotmail.com`;\n",
    "GRANT ALL PRIVILEGES ON EXTERNAL LOCATION etl_silver_location TO `tripathiabhi@hotmail.com`;\n",
    "GRANT ALL PRIVILEGES ON EXTERNAL LOCATION etl_gold_location TO `tripathiabhi@hotmail.com`;\n",
    "GRANT ALL PRIVILEGES ON EXTERNAL LOCATION etl_checkpoint_location TO `tripathiabhi@hotmail.com`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e58bfec-a1d3-4670-9d25-75e101330214",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Access"
    }
   },
   "outputs": [],
   "source": [
    "# Test that you can now access the storage through Unity Catalog\n",
    "# This should work without any spark.conf.set() calls\n",
    "\n",
    "storage_account = \"etldatalakeabhi\"\n",
    "container = \"datalake\"\n",
    "\n",
    "# Test listing the raw directory\n",
    "try:\n",
    "    files = dbutils.fs.ls(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/raw/\")\n",
    "    print(\"✓ Successfully accessed storage!\")\n",
    "    print(f\"\\nFound {len(files)} items in raw/ directory:\")\n",
    "    for file in files[:10]:  # Show first 10 items\n",
    "        print(f\"  - {file.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error accessing storage: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure you completed all Azure Portal steps (Cell 3)\")\n",
    "    print(\"2. Verify storage credential was created (Cell 6)\")\n",
    "    print(\"3. Verify external locations were created (Cell 7)\")\n",
    "    print(\"4. Check that permissions were granted (Cell 8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210d0eca-2609-4c07-9a8f-a1fc987c1429",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Updated Path Variables"
    }
   },
   "outputs": [],
   "source": [
    "# Updated storage paths - no authentication needed!\n",
    "# Unity Catalog handles authentication automatically through external locations\n",
    "\n",
    "storage_account = \"etldatalakeabhi\"\n",
    "container = \"datalake\"\n",
    "\n",
    "raw_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/raw/\"\n",
    "bronze_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/\"\n",
    "silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/\"\n",
    "gold_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/gold/\"\n",
    "checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoint/\"\n",
    "\n",
    "print(\"✓ Storage paths configured:\")\n",
    "print(f\"RAW: {raw_path}\")\n",
    "print(f\"BRONZE: {bronze_path}\")\n",
    "print(f\"SILVER: {silver_path}\")\n",
    "print(f\"GOLD: {gold_path}\")\n",
    "print(f\"CHECKPOINT: {checkpoint_path}\")\n",
    "print(\"\\n✓ No spark.conf.set() needed - Unity Catalog handles authentication!\")\n",
    "print(\"\\nYou can now use these paths in your ETL pipeline without any authentication code.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7064402793545746,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "000_setup_storage",
   "widgets": {
    "cancelled_status": {
     "currentValue": "cancelled",
     "nuid": "7e057937-5c55-4fb1-b04a-e22c7128c31e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "cancelled",
      "label": null,
      "name": "cancelled_status",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "cancelled",
      "label": null,
      "name": "cancelled_status",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "container_name": {
     "currentValue": "datalake",
     "nuid": "1cef48a9-83fd-476f-9567-9125b7a86cf2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "datalake",
      "label": null,
      "name": "container_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "datalake",
      "label": null,
      "name": "container_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "env": {
     "currentValue": "dev",
     "nuid": "36c5e4d6-1698-4b29-8587-bfb1b116c0ef",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "github_url": {
     "currentValue": "https://raw.githubusercontent.com/abhishektripathi27/databricks-etl-pipeline/main/data/orders.csv",
     "nuid": "43208a0c-b0cb-4c25-b32d-cd8a9c09edd6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "https://raw.githubusercontent.com/abhishektripathi27/databricks-etl-pipeline/main/data/orders.csv",
      "label": null,
      "name": "github_url",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "https://raw.githubusercontent.com/abhishektripathi27/databricks-etl-pipeline/main/data/orders.csv",
      "label": null,
      "name": "github_url",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "min_order_amount": {
     "currentValue": "0.01",
     "nuid": "f4ff0fc5-78b0-4c99-baa0-65c9c84728ef",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.01",
      "label": null,
      "name": "min_order_amount",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.01",
      "label": null,
      "name": "min_order_amount",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "storage_account_name": {
     "currentValue": "etldatalakeabhi",
     "nuid": "1d5598cf-4616-4716-ab31-4a136865feda",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "etldatalakeabhi",
      "label": null,
      "name": "storage_account_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "etldatalakeabhi",
      "label": null,
      "name": "storage_account_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
